{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Model Selection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNeargfxbZEJ6NuF3prkvL8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lovegohome/ML/blob/main/wLGM/2_Model_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8nMGid3v6Mz"
      },
      "source": [
        "# 1. Model Selection 모듈 소개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU2mMjsyTrUp"
      },
      "source": [
        "\n",
        "\n",
        "## 학습/테스트 데이터 셋 분리 – train_test_split()\n",
        "\n",
        "### 사이킷런 model_selection 모듈의 주요 기능\n",
        "- 학습 데이터와 테스트 데이터 세트 분리\n",
        "- 교차 검증 분할 및 평가\n",
        "- Estimator의 하이퍼 파라미터 튜닝\n",
        "\n",
        "**학습 데이터와 테스트 데이터 세트 분리**\n",
        "- train_test_split() 함수 사용\n",
        "\n",
        "**학습 데이터 세트**\n",
        "- 머신러닝 알고리즘의 학습을 위해 사용\n",
        "- 데이터의 속성(피처)과 결정값(레이블) 모두 포함\n",
        "- 학습 데이터를 기반으로 머신러닝 알고리즘이 데이터 속성과 결정값의 패턴을 인지하고 학습\n",
        "\n",
        "**테스트 데이터 세트** \n",
        "- 학습된 머신러닝 알고리즘 테스트용\n",
        "- 머신러닝 알고리즘은 제공된 속성 데이터를 기반으로 결정값 예측\n",
        "- 학습 데이터와 별도의 세트로 제공\n",
        "\n",
        "**train_test_split() 함수**\n",
        "\n",
        "train_test_split(feature_dataset, label_dataset, test_size, train_size, random_state, shuffle, stratify)\n",
        "\n",
        "\n",
        "- feature_dataset : 피처 데이터 세트\n",
        "    - 피처(feature)만으로 된 데이터(numpy) [5.1, 3.5, 1.4, 0.2],...\n",
        "- label_dataset : 레이블 데이터 세트\n",
        "    - 레이블(결정 값) 데이터(numpy) [0 0 0 ... 1 1 1 .... 2 2 2]\n",
        "- label_dataset : 테스트 데이터 세트 비율\n",
        "    - 전체 데이터 세트 중 테스트 데이터 세트 비율\n",
        "    - 지정하지 않으면 0.25\n",
        "- random_state : 세트를 섞을 때 해당 int 값을 보고 섞음\n",
        "    - 수행할 때마다 동일한 데이터 세트로 분리하기 위해 시드값 고정(실습용)\n",
        "    - 0 또는 4가 가장 많이 사용\n",
        "    - 하이퍼 파라미터 튜닝시 이 값을 고정해두고 튜닝해야 매번 데이터셋이 변경되는 것을 방지할 수 있음\n",
        "    \n",
        "- shuffle : 분할하기 전에 섞을지 지정\n",
        "    - default=True (보통은 default 값으로 놔둠)\n",
        "- stratify : 지정된 레이블의 클래스 비율에 맞게 분할\n",
        "    - default=None\n",
        "    - classification을 다룰 때 매우 중요한 옵션값\n",
        "    - stratify 값을 target으로 지정해주면 각각의 class 비율(ratio)을 train/ validation에 유지해 줌(한 쪽에 쏠려서 분배되는 것을 방지)\n",
        "    - 이 옵션을 지정해 주지 않고 classification 문제를 다룬다면, 성능의 차이가 많이 날 수 있음\n",
        "    \n",
        "   \n",
        "   \n",
        "예. train_test_split(iris_data, iris_label, test_size=0.3, random_state=11)\n",
        "\n",
        "\n",
        "train_test_split() 반환값\n",
        "* X_train : 학습용 피처 데이터 세트 (feature)\n",
        "* X_test : 테스트용 피처 데이터 세트 (feature)\n",
        "* y_train : 학습용 레이블 데이터 세트 (target)\n",
        "* y_test : 테스트용 레이블 데이터 세트 (target)\n",
        "* feature : 대문자 X_\n",
        "* label(target) : 소문자 y_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJou8fY6VGhQ"
      },
      "source": [
        "### (1) 학습/테스트 데이터 셋 분리하지 않고 예측\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zVF3sluToe8",
        "outputId": "9c1aa976-4f11-4185-af24-5755a4c09ace"
      },
      "source": [
        "# (1) 학습/테스트 데이터 셋 분리하지 않고 예측\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris_data = load_iris()\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "\n",
        "train_data = iris_data.data\n",
        "train_label = iris_data.target\n",
        "\n",
        "# 학습 수행 \n",
        "dt_clf.fit(train_data, train_label)\n",
        "\n",
        "# 테스트\n",
        "pred = dt_clf.predict(train_data)\n",
        "print(\"예측 정확도 : \", accuracy_score(train_label, pred))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "예측 정확도 :  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKV1-0IcVNBq"
      },
      "source": [
        "- 예측을 train_data로 했기 때문에 결과 1.0 (100%)으로 출력 (잘못됨)\n",
        "- 예측은 테스트 데이터로 해야 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf76o-ddVHMZ"
      },
      "source": [
        "### (2) 학습/테스트 데이터 셋 분리하고 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGoHn5aWUQFn"
      },
      "source": [
        "# (2) 학습/테스트 데이터 셋 분리하고 예측 \n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris_data = load_iris()\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "\n",
        "# 학습/테스트 분할(split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target,\n",
        "                test_size=0.2, random_state=4)\n",
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuv0fQaYWJ_Z"
      },
      "source": [
        "# 학습 수행\n",
        "dt_clf.fit(X_train, y_train)\n",
        "\n",
        "# 예측 수행\n",
        "pred = dt_clf.predict(X_test)\n",
        "print(\"예측 정확도 : \", accuracy_score(y_test, pred))\n",
        "\n",
        "# random_state=21 어떻게주느냐에 따라 예측 정확도가 조금씩 \n",
        "# 달라진다. \n",
        "# test_size=0.2 도 어떻게 주느냐에 따라 값이 달라짐\n",
        "# 손 맛 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB_0qox9WLg4"
      },
      "source": [
        "# 넘파이 ndarray 뿐만 아니라 판다스 DataFrame/Series도 train_test_split( )으로 분할 가능\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "iris_df = pd.DataFrame(iris_data.data, \n",
        "                       columns = iris_data.feature_names)\n",
        "iris_df['target'] = iris_data.target\n",
        "iris_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-awtSM8WM3g"
      },
      "source": [
        "# 피처 데이터프레임 반환(마지막 열 전까지, 마지막 열 제외)\n",
        "feature_df = iris_df.iloc[:, :-1]\n",
        "\n",
        "# 타겟 데이터프레임 반환\n",
        "target_df = iris_df.iloc[:, -1]\n",
        "\n",
        "# 학습 / 테스트 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_df, \n",
        "                                                    target_df,\n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=4)\n",
        "print(y_train)\n",
        "# 즉, 판다스로도 분할 되는 걸 확인함. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbyC_azLWPZm"
      },
      "source": [
        "type(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-hwu43oWQIw"
      },
      "source": [
        "dt_clf = DecisionTreeClassifier()\n",
        "dt_clf.fit(X_train, y_train)\n",
        "pred = dt_clf.predict(X_test)\n",
        "print('예측정확도: {0: .3f}'.format(accuracy_score(y_test, pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD0tlVWuWV2x"
      },
      "source": [
        "## Data Split과 모델 검증\n",
        "\n",
        "- 언제\n",
        "    - \"충분히 큰\" 데이터 세트를 가용할 때\n",
        "    - \"충분히 큰\" 데이터가 없을 때에는 교차 확인(Cross Validation) 고려\n",
        "    \n",
        "\n",
        "- 왜\n",
        "    - 학습에 사용되지 않은 데이터를 사용하여 예측을 수행함으로써 모델의 일반적인 성능에 대한 적절한 예측을 함\n",
        "    \n",
        "\n",
        "- 어떻게\n",
        "    - 홀드-아웃(Hold-out)\n",
        "    - 교차검증(Cross Validation,CV)\n",
        "    - 필요에 따라 Stratified Sampling\n",
        "\n",
        "### 홀드-아웃 방식\n",
        "- 데이터를 두 개 세트로 나누어 각각 Train과 Test 세트로 사용\n",
        "- Train과 Test의 비율을 7:3 ~ 9:1로 널리 사용하나, 알고리즘의 특성 및 상황에 따라 적절한 비율을 사용\n",
        "- Train – Validation - Test로 나누기도 함\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "https://algotrading101.com/learn/train-test-split-2/\n",
        "\n",
        "\n",
        "부적합한 데이터 선별로 인한 문제점\n",
        "- ML은 데이터에 기반하고, \n",
        "- 데이터는 이상치, 분포도, 다양한 속성값, 피처 중요도 등 \n",
        "- ML에 영향을 미치는 다양한 요소를 가지고 있음\n",
        "- 특정 ML 알고리즘에 최적으로 동작할 수 있도록\n",
        "- 데이터를 선별해서 학습한다면\n",
        "- 실제 데이터 양식과는 많은 차이가 있을 것이고\n",
        "- 결국 성능 저하로 이어질 것임\n",
        "\n",
        "문제점 개선 ---> 교차 검증을 이용해 더 다양한 학습 평가 수행\n",
        "\n",
        "### 교차검증(Cross Validation, CV)\n",
        "- k-fold Cross Validation이라고도 함\n",
        "- 전체 데이터 세트를 임의로 k개의 그룹으로 나누고, 그 가운데 하나의 그룹을 돌아가면서 테스트 데이터 세트로, 나머지 k-1개 그룹은 학습용 데이터 세트로 사용하는 방법\n",
        "- 별도의 여러 세트로 구성된 학습 데이터 세트와 검증 데이터 세트에서 학습과 평가를 수행\n",
        "\n",
        "\n",
        "- 사용 목적\n",
        "    - 데이터에 적합한 알고리즘인지 평가하기 위해 \n",
        "    - 모델에 적절한 hyperparameter 찾기 위해\n",
        "    - 과대적합 예방\n",
        "    - 데이터 편중을 막기 위해\n",
        "    \n",
        "\n",
        "    \n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "\n",
        "http://karlrosaen.com/ml/learning-log/2016-06-20/\n",
        "\n",
        "### 교차 검증 방법\n",
        "- K 폴드 교차 검증\n",
        "- Stratified K 폴드 교차 검증\n",
        "\n",
        "### K 폴드 교차 검증\n",
        "- K개의 데이터 폴드 세트를 만들어서\n",
        "- K번만큼 각 폴드 세트에 학습과 검증 평가를 반복적으로 수행\n",
        "- 가장 보편적으로 사용되는 교차 검증 기법\n",
        "\n",
        "\n",
        "- 5-폴드 교차 검증\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "**K 폴드 교차 검증 프로세스 구현을 위한 사이킷런 클래스**\n",
        "\n",
        "(1) KFold 클래스 : 폴드 세트로 분리하는 객체 생성\n",
        "- kfold = KFold(n_splits=5)\n",
        "\n",
        "(2) split() 메소드 : 폴드 데이터 세트로 분리\n",
        "- kfold.split(features)\n",
        "- 각 폴드마다  \n",
        "    학습용, 검증용, 테스트 데이터 추출  \n",
        "    학습용 및 예측 수행  \n",
        "    정확도 측정  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXSxPbDEWYPx"
      },
      "source": [
        "(3) 최종 평균 정확도 계산\n",
        "* K 폴드 예제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiF9nBrGWap5"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "iris = load_iris()\n",
        "features = iris.data\n",
        "label = iris.target\n",
        "\n",
        "#iris.shape \n",
        "#features.shape[0]\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ccRzFCRiZYN"
      },
      "source": [
        "# DecisionTreeClassifier 객체 생성 \n",
        "dr_clf = DecisionTreeClassifier(random_state=156)\n",
        "# 5개의 폴드 세트로 분리하는 KFold 객체 생성\n",
        "kfold = KFold(n_splits=5)\n",
        "# 폴드 세트별 정확도를 담을 리스트 객체 생성\n",
        "cv_accuracy =  [] # 빈리스트 만들고 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbtaeylHiaj0"
      },
      "source": [
        "kfold.split(features) # 짠 나눠짐 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM1J1odkicMc"
      },
      "source": [
        "# 폴드 별 학습용, 검증용 데이터 세트의 행 인덱스 확인\n",
        "for train_index, test_index in kfold.split(features):\n",
        "    print(train_index, test_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vhVefTZic7_"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "for train_index, test_index in kfold.split(features):\n",
        "    X_train, X_test = features[train_index], features[test_index]\n",
        "    y_train = label[train_index]\n",
        "    y_test = label[test_index]\n",
        "    \n",
        "    dt_clf.fit(X_train, y_train) # 학습\n",
        "    # 강사님 dr_clf로 객체 생성하신거같아요~??\n",
        "    pred = dt_clf.predict(X_test) # 앞에서 5번 나눈 거 대로\n",
        "    \n",
        "#    acc = accuracy_score(y_test, pred)\n",
        "    acc = np.round(accuracy_score(y_test, pred), 3) # 소수점 조절 \n",
        "    train_size = X_train.shape[0]\n",
        "    test_size = X_test.shape[0]\n",
        "    \n",
        "    print('정확도: %f, 학습데이터 크기: %d, 검증데이터 크기: %d,' \n",
        "          %(acc, train_size, test_size))\n",
        "    cv_accuracy.append(acc) \n",
        "    \n",
        "print('평균 검증 정확도: ', np.mean(cv_accuracy).round(1))\n",
        "\n",
        "# 옴... cv_accuaracy\n",
        "# accuracy???\n",
        "# 왜 교수님은 0.9로 뜨는데 난 왜이렇게 길게 나오지..?\n",
        "\n",
        "# 데이터 세트가 어떻게 구성되느냐에 따라 다른 평균값이 나오기 때문에\n",
        "# 생기는 차이가 아닐까 합니다.\n",
        "# print('평균 검증 정확도 : ', np.mean(cv_accuracy).round(1))\n",
        "# 이렇게 바꾸시면 0.9\n",
        "# 위에서 round 처리 한 것은 acc의 값이지 \n",
        "# cv_accuracy의 값이 아니기 때문에 cv_accuracy의 출력에는 \n",
        "# 영향을 미치지 않습니다.\n",
        "\n",
        "# random_state가 모든 PC가 완벽히 같은 값을 뽑게 해주는게 아니라\n",
        "# 거의 같은 값을 뽑게 해주는거라서 차이가 있는 거다..?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56aY3MSCi2bX"
      },
      "source": [
        "### Stratified K 폴드 교차 검증\n",
        "- 불균형한 분포도를 가진 레이블(결정 클래스) 데이터 집합을 위한 K 폴드 방식\n",
        "\n",
        "### 불균형한 데이터(imbalanced data) 문제\n",
        "- 관심 대상 데이터가 상대적으로 매우 적은 비율로 나타나는 데이터 문제\n",
        "\n",
        "- 분류 문제인 경우 : 클래스들이 균일하게 분포하지 않은 문제를 의미\n",
        "    - 예. 불량률이 1%인 생산라인에서 양품과 불량품을 예측하는 문제\n",
        "    - 사기감지탐지(fraud detection), 이상거래감지(anomaly detection), 의료진단(medical diagnosis) 등 에서 자주 나타남\n",
        "\n",
        "- 회귀 문제인 경우 : 극단값이 포함되어 있는 \"치우친\" 데이터 사례\n",
        "    - 예. 산불에 의한 피해 면적을 예측하는 문제\n",
        "    (https://www.kaggle.com/aleksandradeis/regression-addressing-extreme-rare-cases)\n",
        "\n",
        "\n",
        "**우회/극복하는 방법**\n",
        "- 데이터 추가 확보\n",
        "\n",
        "\n",
        "- Re-Sampling\n",
        "    - Under-sampling(과소표집)\n",
        "        - 다른 클래스에 비하여 상대적으로 많이 나타나는 클래스의 개수를 줄임\n",
        "        - 균형은 유지할 수 있으나 유용한 정보에 대한 손실이 있을 수 있음\n",
        "    - Over-Sampling(과대표집)\n",
        "        - 상대적으로 적게 나타나는 클래스의 데이터를 복제하여 데이터의 개수를 늘림\n",
        "        - 정보 손실은 없이 학습 성능은 높아지는 반면, 과적합의 위험이 있음\n",
        "        - 이를 회피하기 위해서 SMOTE 와 같이 임의의 값을 생성하여 추가하는 방법 사용\n",
        "        \n",
        "        \n",
        "![image.png](attachment:image.png)    \n",
        "\n",
        "* 먼저 K 폴드 문제점 확인하고, \n",
        "* 사이킷런의 Stratified K 폴드 교차 검증 방법으로 개선\n",
        "* 붓꽃 데이터 세트를 DataFrame으로 생성하고 \n",
        "* 레이블 값의 분포도 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilq2x4nxi36c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "irisf_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "iris_df['label']=iris.target\n",
        "iris_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dmNSlJNic-i"
      },
      "source": [
        "iris_df['label'].value_counts()\n",
        "# 레이블 값은 0, 1, 2 값 모두 50개로 동일\n",
        "# 즉, Setosa, Versicolor, virginica 각 품종 50개 씩 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v66naivPidBB"
      },
      "source": [
        "kfold = KFold(n_splits=3)\n",
        "\n",
        "n = 0\n",
        "for train_index, test_index in kfold.split(iris_df):\n",
        "    n += 1 \n",
        "    label_train = iris_df['label'].iloc[train_index]\n",
        "    label_test = iris_df['label'].iloc[test_index]\n",
        "    print(\"[교차검증: %d]\" %(n))\n",
        "    print(\"  학습용 : \\n \", label_train.value_counts())\n",
        "    print(\"  검증용 : \\n \", label_test.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwvLsqsjidFQ"
      },
      "source": [
        "########## 참고 : 3개의 폴드 세트로 KFold 교차 검증 : 정확도 : 0 ###########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqam3nZ4idHZ"
      },
      "source": [
        "# DecisionTreeClassifier 객체 생성\n",
        "dt_clf = DecisionTreeClassifier(random_state=156)\n",
        "\n",
        "# 3개의 폴드 세트로 분리하는 KFold 객체 생성\n",
        "kfold = KFold(n_splits=3)\n",
        "\n",
        "# 폴드 세트별 정확도를 담을 리스트 객체 생성\n",
        "cv_accuracy = []\n",
        "\n",
        "n=0\n",
        "for train_index, test_index in kfold.split(iris_df):\n",
        "    X_train, X_test = features[train_index], features[test_index]\n",
        "    y_train, y_test = label[train_index], label[test_index]\n",
        "    \n",
        "    # 학습 및 예측\n",
        "    dt_clf.fit(X_train, y_train)\n",
        "    pred = dt_clf.predict(X_test)\n",
        "    n += 1 \n",
        "    \n",
        "    # 반복 시 마다 정확도 측정\n",
        "    acc = np.round(accuracy_score(y_test, pred), 3)\n",
        "    train_size = X_train.shape[0]\n",
        "    test_size = X_test.shape[0]\n",
        "    print('%d \\n정확도: %f, 학습데이터크기: %d, 검증데이터: %d'\n",
        "         %(n, acc, train_size, test_size))\n",
        "    \n",
        "    cv_accuracy.append(accuracy)\n",
        "    \n",
        "# 개별 iteration별 정확도를 합하여 평균 정확도 계산\n",
        "print('\\n## 평균 검증 정확도: ', np.mean(cv_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD2e4LCdidJ9"
      },
      "source": [
        "########## 참고 끝   ###########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW61IuZ7mL6u"
      },
      "source": [
        "- 위 코드 결과의 문제점\n",
        "    - 학습하지 않은 데이터를 검증 데이터로 사용\n",
        "    - 원할한 학습과 예측이 어려움\n",
        "    - 검증 정확도는 0\n",
        "\n",
        "StratifiedKFold 클래스\n",
        "- 원본 데이터의 레이블 분포를 고려한 뒤 이 분포와 동일하게 학습과 검증데이터 세트를 분배\n",
        "\n",
        "- KFold 사용법과 거의 비슷\n",
        "- 차이점\n",
        "  - 레이블 데이터 분포도에 따라 학습/검증 데이터를 나누기 때문에\n",
        "  - split() 메서드에 인자로 피처 데이터 세트뿐 아니라 \n",
        "  - 레이블 데이터 세트도 반드시 필요하다는 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCswVPkkmKSp"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=3)\n",
        "n=0\n",
        "\n",
        "for train_index, test_index in skf.split(iris_df, iris_df['label']):\n",
        "    n = n + 1\n",
        "    \n",
        "    label_train = iris_df['label'].iloc[train_index]\n",
        "    label_test = iris_df['label'].iloc[test_index]\n",
        "    print(\"[교차검증 : %d]\" %n)\n",
        "    print('학습용 레이블 분포: \\n', label_train.value_counts())\n",
        "    print('검증용 레이블 분포: \\n', label_test.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4SnvF1GmKEr"
      },
      "source": [
        "# StratifiedfKFold를 이용해 붓꽃 데이터 교차 검증\n",
        "dt_clf = DecisionTreeClassifier(random_state = 156)\n",
        "\n",
        "# 3개의 폴드 세트로 분리하는 StratifiedKFold 객체 생성\n",
        "skfold = StratifieldKFold(n_splits=3)\n",
        "\n",
        "# 폴드 세트별 정확도를 담을 리스트 객체 생성\n",
        "cv_accuracy = []\n",
        "\n",
        "n=0\n",
        "for train_index, test_index in skfold.split(features, label):\n",
        "    X_train, X_test = features[train_index], features[test_index]\n",
        "    y_train, y_test = label[train_index], label[test_index]\n",
        "    \n",
        "    # 학습 및 예측\n",
        "    dt_clf.fit(X_train, y_train)\n",
        "    pred = dt_clf.predict(X_test)\n",
        "    \n",
        "    # 반복 시 마다 정확도 측정\n",
        "    n += 1 \n",
        "    \n",
        "    acc = np.round(accuracy_score(y_test, pred), 3)\n",
        "    train_size = X_train.shape[0]\n",
        "    test_size = X_test.shape[0]\n",
        "    print('%d \\n정확도: %f, 학습데이터: %d, 검증데이터크기: %d'\n",
        "         %(n, acc, train_size, test_size))\n",
        "    \n",
        "    cv_accuracy.append(acc)\n",
        "\n",
        "# 개별 iteration별 정확도를 합하여 평균 정확도 계산\n",
        "print('\\n## 평균 검증 정확도: ', np.mean(cv_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpkVVRYJpkcf"
      },
      "source": [
        "Stratified K 폴드의 경우\n",
        "- 원본 데이터의 레이블 분포도 특성을 반영한 학습 및 검증 데이터 세트를 만들 수 있으므로\n",
        "- 왜곡된 레이블 데이터 세트에서는 반드시 Stratified K 폴드를 이용해서 교차 검증해야 함\n",
        "- 일반적으로 분류(Classification)에서의 교차 검증은 K 폴드가 아니라 Stratified K 폴드로 분할되어야 함\n",
        "- 회귀(Regression)에서는 Stratified K 폴드 지원되지 않음\n",
        "    - 이유 : 회귀의 결정값은 이산값 형태의 레이블이 아니라 연속된 숫자값이기 때문에\n",
        "    - 결정값별로 분포를 정하는 의미가 없기 때문\n",
        "\n",
        "## 교차검증을 보다 간편하게 \n",
        "\n",
        "- 교차 검증 (Cross Validation) 과정\n",
        "    1. 폴드 세트 설정\n",
        "    2. for 문에서 반복적으로 학습 및 검증 데이터 추출 및 학습과 예측 수행\n",
        "    3. 폴드 세트별로 예측 성능을 평균하여 최종 성능 평가\n",
        "\n",
        "### cross_val_score( ) 함수\n",
        "- 1 ~ 3 단계의 교차 검증 과정을 한꺼번에 수행\n",
        "- 내부에서 Estimator를 학습(fit), 예측(predict), 평가(evaluation) 시켜주므로\n",
        "- 간단하게 교차 검증 수행 가능\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl6ofTXBpmGD"
      },
      "source": [
        "### 붓꽃 자료를 3개 폴드로 분할하여 학습 및 검증"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXIi16dsmHA5"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score, cross_validate\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "dt_clf = DecisionTreeClassifier(random_state=156)\n",
        "\n",
        "features = iris.date\n",
        "label = iris.target\n",
        "\n",
        "scores = cross_val_score(dt_clf, features, label, scoring='accuracy'\n",
        "                        ,cv = 3)\n",
        "print('교차 검증별 정확도: ', scores)\n",
        "print('평균 검증 정확도: ', np.round(np.mean(scores), 4))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5aYbcQ3qtbc"
      },
      "source": [
        "- cross_val_score()는 cv로 지정된 횟수만큼\n",
        "- scoring 파라미터로 지정된 평가 지표로 평가 결과값을 배열로 반환\n",
        "- 일반적으로 평가 결과값 평균을 평가 수치로 사용\n",
        "\n",
        "## 교차 검증과 최적의 하이퍼파라미터 튜닝을 한번에\n",
        "\n",
        "하이퍼파라미터(Hyper parameter)\n",
        "- 머신러닝 알고리즘을 구성하는 요소\n",
        "- 이 값들을 조정해 알고리즘의 예측 성능을 개선할 수 있음\n",
        "\n",
        "### 사이킷런의 GridSearchCV클래스\n",
        "\n",
        "- Classifier나 Regressor와 같은 알고리즘에 사용되는\n",
        "- 하이퍼 파라미터를 순차적으로 입력하면서\n",
        "- 최적의 파라미터를 편리하게 도출할 수 있는 방법 제공  \n",
        "-(Grid는 격자라는 의미 : 촘촘하게 파라미터를 입력하면서 테스트 하는 방식)\n",
        "\n",
        "즉,  \n",
        "- 머신러닝 알고리즘의 여러 하이퍼 파라미터를  \n",
        "- 순차적으로 변경하면서 최고 성능을 가지는 파라미터를 찾고자 한다면  \n",
        "- 파라미터의 집합을 만들어 순차적으로 적용하면서 최적화 수행  \n",
        "\n",
        "**GridSearchCV 클래스 생성자의 주요 파라미터**\n",
        "\n",
        "- estimator : classifier, regressor, peipeline\n",
        "\n",
        "\n",
        "- param_grid : key + 리스트 값을 가지는 딕셔너리 (estimator 튜닝을 위한 하이퍼 파라미터 )\n",
        "     - key: 파라미터명, 리스트값:파라미터 값\n",
        "     \n",
        "     \n",
        "- scoring : 예측 성능을 측정할 평가 방법 \n",
        "     - 성능 평가 지표를 지정하는 문자열\n",
        "     - 예: 정확도인 경우 'accuracy'\n",
        "     \n",
        "     \n",
        "- cv : 교차 검증을 위해 분할되는 학습/테스트 세트의 개수\n",
        "\n",
        "\n",
        "- refit : 최적의 하이퍼 파라미터를 찾은 뒤 입력된 estimator 객체를 해당 하이퍼 파라미터로 재학습 여부\n",
        "     - 디폴트 : True    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xbaGTx5idMD"
      },
      "source": [
        "# GridSearchCV를 이용해\n",
        "# 결정 트리 알고리즘의 여러 가지 최적화 파라미터를 순차적으로 적용해서\n",
        "# 붓꽃 데이터 예측 분석\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data,\n",
        "                                                   iris.target,\n",
        "                                                   test_size = 0.2,\n",
        "                                                   random_state=121)\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "\n",
        "parameters = {'max_depth': [1,2,3], 'min_samples_split': [2,3]}\n",
        "# 하이퍼파라미터는 딕셔너리 형식으로 지정\n",
        "# key : 결정트리의 하이파라미터\n",
        "# value : 하이퍼파라미터의 값"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYephMnEr489"
      },
      "source": [
        "min_samples_split : 자식 규칙 노드를 분할해서 만드는데 필요한 최소 샘플 데이터 개수\n",
        "- min_samples_split=4로 설정하는 경우\n",
        "    - 최소 샘플 개수가 4개 필요한데\n",
        "    - 3개만 있는 경우에는 더 이상 자식 규칙 노드를 위한 분할을 하지 않음\n",
        "\n",
        "\n",
        "트리 깊이도 줄어서 더 간결한 결정 트리 생성\n",
        "\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8heteX0dr6DF"
      },
      "source": [
        "grid_tree = GridSearchCV(dt_clf, param_grid=parameters, cv=3, \n",
        "                         refit=True, return_train_score=True)\n",
        "grid_tree.fit(X_train, y_train)\n",
        "\n",
        "scores_df = pd.DataFrame(grid_tree.cv_results_)\n",
        "scores_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qX7OsjSidOW"
      },
      "source": [
        "# 파라미터 확인\n",
        "grid_tree.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_QpHUk8idRO"
      },
      "source": [
        "# GridSearchCV 결과 세트로 딕셔너리 형태인 cv_results_를 \n",
        "# DataFrame으로 변환 후 일부 파라미터 확인 \n",
        "scores_df[['params', 'mean_test_score', 'rank_test_score']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74cW0x8YidSh"
      },
      "source": [
        "# 최고 성능을 가지는 파라미터 조합 및 예측 성능 1위 값 출력\n",
        "print('최적 파라미터: ', grid_tree.best_params_)\n",
        "print('최고 정확도: ', grid_tree.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_QaqB4LidVE"
      },
      "source": [
        "# GridSearchCV 객체의 생성 파라미터로 refit = True로 설정된 경우(디폴트)\n",
        "# GridSearchCV가 최적 성능을 나타내는 하이퍼 파리미터로 Estimator를 학습하고\n",
        "# best_estimator_ 로 저장\n",
        "# (GridSearchCV의 refit으로 이미 학습이 된 estimator)\n",
        "best_dt = grid_tree.best_estimator_\n",
        "\n",
        "# best_estimator_는 이미 최적 학습이 됐으므로 별도 학습 필요 없이\n",
        "# 바로 예측 가능\n",
        "\n",
        "pred = best_dt.predict(X_test)\n",
        "accuracy_score(y_test, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axslYnKuuEPA"
      },
      "source": [
        "**일반적인 머신러닝 모델 적용 방법**\n",
        "\n",
        "- 일반적으로 학습 데이터를 GridSearchCV를 이용해\n",
        "- 최적 하이퍼 파라미터 튜닝을 수행한 뒤에\n",
        "- 별도의 테스트 세트에서 이를 평가하는 방식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6CqDqyhv-NK"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxOYxI6UwVWe"
      },
      "source": [
        "- 데이터를 분석하기에 좋은 형태로 만드는 과정\n",
        "- Garbage In Garbage Out\n",
        "    - 데이터 전처리가 중요한 근본적인 이유\n",
        "    - 데이터 품질은 분석 결과 품질의 출발점\n",
        "    \n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "### 데이터 전처리의 필요성\n",
        "- 데이터 품질이 높은 경우에도 전처리 필요\n",
        "    - 구조적 형태가 분석 목적에 적합하지 않은 경우\n",
        "    - 사용하는 툴, 기법에서 요구하는 데이터 형태\n",
        "    - 데이터가 너무 많은 경우\n",
        "    - 데이터 분석의 레벨이 데이터 저장 레벨과 다른 경우\n",
        "\n",
        "\n",
        "- 데이터 품질을 낮추는 요인\n",
        "    - 불완전(incomplete) : 데이터 필드가 비어 있는 경우\n",
        "    - 잡음(noise) : 데이터에 오류가 포함된 경우\n",
        "    - 모순(inconsistency) : 데이터 간 정합성, 일관성이 결여된 경우\n",
        "\n",
        "### 데이터 전처리 주요 기법\n",
        "- 데이터 정제\n",
        "    - 결측치, 이상치, 잡음\n",
        "\n",
        "\n",
        "- 데이터 결합\n",
        "\n",
        "\n",
        "- 데이터 변환\n",
        "    - Normalization, scaling\n",
        "\n",
        "\n",
        "- 차원 축소\n",
        "    - Feature selection\n",
        "        - filter, wrapper, embedded\n",
        "    - Feature extraction\n",
        "        - PCA, SVD, FA, NMF\n",
        "\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "### 결측값(missing value) 처리\n",
        "- 해당 데이터 행을 모두 제거(완전제거법)\n",
        "- 수작업으로 채워 넣음\n",
        "- 특정값 사용\n",
        "- 핫덱(hot-deck) 대체법\n",
        "    - 동일한 조사에서 다른 관측값으로부터 얻은 자료를 이용해 대체\n",
        "    - 관측값 중 결측치와 비슷한 특성을 가진 것을 무작위 추출하여 대체\n",
        "- 평균값 사용 (전체 평균 혹은 기준 속성 평균)\n",
        "    - 표준오차 과소추정 발생\n",
        "- 가장 가능성이 높은 값 사용 (회귀분석, 보간법 등)\n",
        "\n",
        "## 데이터 인코딩\n",
        "\n",
        "- 문자열을 숫자형으로 변환\n",
        "\n",
        "\n",
        "- 인코딩 방식\n",
        "    - 레이블 인코딩(Label encoding)\n",
        "    - 원-핫 인코딩(One-hot encoding)\n",
        "\n",
        "### 레이블 인코딩 (Label encoding)\n",
        "- 문자열 데이터를 숫자로 코드화\n",
        "- 범주형 자료의 수치화\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO4TJLl-wcL5"
      },
      "source": [
        "**사이킷런의 레이블 인코딩 클래스 : LabelEncoder**\n",
        "1. LabelEncoder 객체 생성\n",
        "2. fit() 메서드\n",
        "    - 레이블 인코더를 맞춤\n",
        "3. transform() 메서드\n",
        "    - 인코딩된 레이블 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYq08R_hwfU5"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "items = ['TV', '냉장고', '전자렌지', '컴퓨터', '선풍기', '선풍기',\n",
        "       '믹서', '믹서']\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "labels = encoder.transform(items)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdWoy_9KFw2q"
      },
      "source": [
        "# LabelEncoder 객체의 classes_ : 인코딩된 문자열 값 목록 확인\n",
        "# 인코딩 전 원래의 값 확인 : encoder.classes_ 속성\n",
        "encoder.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MmqWY45FxC6"
      },
      "source": [
        "# inverse_transform() : 인코딩된 값을 다시 디코딩\n",
        "# 인코딩된 값 디코딩\n",
        "encoder.inverse_transform([3,0,2,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJiu4QAvF4QC"
      },
      "source": [
        "- 레이블 인코딩된 데이터른 회귀와 같은 ML 알고리즘에 적용해서는 안됨\n",
        "    - 변환된 수치의 크기가 의미가 없기 때문에\n",
        "\n",
        "### 원-핫 인코딩(One-Hot encoding)\n",
        "\n",
        "- feature 값의 유형에 따라 새로운 feature를 추가하여\n",
        "- 고유 값에 해당하는 컬럼만 1을 표시하고 나머지 컬럼에는 0을 표시\n",
        "\n",
        "- 범주형 변수를 독립변수로 갖는 회귀분석의 경우 범주형 변수를 dummy 변수로 변환\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIXHlUG4F9GK"
      },
      "source": [
        "**사이킷런에서 원-핫 인코딩 클래스 : OneHotEncoder**\n",
        "\n",
        "**원-핫 인코딩 변환 과정**\n",
        "1. 문자열 값을 숫자형 값으로 변환\n",
        "2. 입력 값을 2차원 데이터로 변환\n",
        "3. OneHotEncoder 클래스로 원-핫 인코딩 적용\n",
        "    - fit()\n",
        "    - transform()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzaFeD8vFxFq"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "items = ['TV', '냉장고', '전자렌지', '컴퓨터', '선풍기', '선풍기',\n",
        "       '믹서', '믹서']\n",
        "\n",
        "# 1. 먼저 숫자값으로 변환을 위해 LabelEncoder로 변환\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "labels = encoder.transform(items)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6GidaO_FxJG"
      },
      "source": [
        "# 2. 2차원 데이터로 변환\n",
        "labels = labels.reshape(-1,1) #-1: 모든 행, 1은 열 1개 # ( , 1)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdktiD0cFxLG"
      },
      "source": [
        "# 3. 원-핫 인코딩을 적용\n",
        "one_encoder = OneHotEncoder()\n",
        "one_encoder.fit(labels)\n",
        "one_labels = one_encoder.transform(labels)\n",
        "one_labels\n",
        "# sparse matrix : 희소행렬 (행렬의 값이 대부분 0인 경우)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54RCc6AxFxN3"
      },
      "source": [
        "print(one_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EiL1_8eFxQp"
      },
      "source": [
        "one_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoYwetkRRbiG"
      },
      "source": [
        "# 2차원 형태로 출력\n",
        "one_labels.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a9Cuzt6RbmQ"
      },
      "source": [
        "# 원-핫 인코딩 전체 과정\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "items=['TV', '냉장고', '전자렌지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']\n",
        "\n",
        "# 1. 먼저 숫자값으로 변환을 위해 LabelEncoder로 변환\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "labels = encoder.transform(items)\n",
        "\n",
        "# 2. 2차원 데이터로 변환합니다. \n",
        "labels = labels.reshape(-1,1)\n",
        "\n",
        "# 3. 원-핫 인코딩을 적용합니다.\n",
        "oh_encoder = OneHotEncoder()\n",
        "oh_encoder.fit(labels)\n",
        "oh_labels = oh_encoder.transform(labels)\n",
        "\n",
        "print('원-핫 인코딩 데이터')\n",
        "print(oh_labels.toarray())\n",
        "print('원-핫 인코딩 데이터 차원')\n",
        "print(oh_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX__B5wyRmv9"
      },
      "source": [
        "### Pandas API 사용 원-핫 인코딩 수행\n",
        "- get_dummies() 메서드 사용\n",
        "- 숫자형으로 변환없이 바로 변환\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H8kbHagRbpY"
      },
      "source": [
        "### Pandas API 사용 원-핫 인코딩 수행\n",
        "- get_dummies() 메서드 사용\n",
        "- 숫자형으로 변환없이 바로 변환"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxgRTbAQRbsJ"
      },
      "source": [
        "pd.get_dummies(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YndtOoQCRbvP"
      },
      "source": [
        "# Pandas 데이터프레임을 Numpy 배열로 변환\n",
        "pd.get_dummies(df).to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZpgrJSfSMmX"
      },
      "source": [
        "### 피처 스케일링과 정규화\n",
        "\n",
        "- 피처 스케일링(feature scaling)\n",
        "    - 서로 다른 변수의 값 범위를 일정한 수준으로 맞춤\n",
        "\n",
        "\n",
        "- 방식\n",
        "    - Z-scaling\n",
        "        - 표준화(standardization)\n",
        "        - 평균이 0이고 분산이 1인 가우지안 정규분포로 변환\n",
        "        - 정규화(Normalization)\n",
        "        - sklearn.preprocessing의  StandardScaler 모듈\n",
        "\n",
        "    - Min-max\n",
        "        -  0~1로 변환\n",
        "        - sklearn.preprocessing의  MinMaxScaler 모듈\n",
        "\n",
        "    - 벡터 정규화\n",
        "        - Sklearn의 Nomalizer 모듈\n",
        "        - 선형대수의 정규화 개념\n",
        "        - 개별 벡터의 크기를 맞추기 위해 모든 피처벡터의 크기로 나누어 변환\n",
        "\n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldYBfJCjSP9A"
      },
      "source": [
        "StandardScaler\n",
        "* 표준화 지원 클래스\n",
        "* 개별 피처를 평균이 0이고 분산이 1인 값으로 변환\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0shfZEzSQ-w"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "# 붓꽃 데이터 셋을 로딩하고 DataFrame으로 변환\n",
        "\n",
        "iris = load_iris()\n",
        "iris_data = iris.data\n",
        "iris_df = pd.DataFrame(data=iris_data, columns = iris.feature_names)\n",
        "\n",
        "print(iris_df.mean())\n",
        "print(iris_df.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ofsuDh5SsBe"
      },
      "source": [
        "StandardScaler 이용 표준화해서 변환\n",
        "1.  StandardScaler 객체 생성\n",
        "2. fit() : 데이터 변환을 위한 기준 정보 설정\n",
        "3. transform() : fit()에서 설정된 정보를 이용해 데이터 변환\n",
        "    - scale 변환된 데이터 셋이 numpy ndarry로 반환\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bySdpd0gSra-"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(iris_df)\n",
        "iris_scaled = scaler.transform(iris_df)\n",
        "# print(iris_scaled.mean()) #e의 -15승. 거의 0 \n",
        "# print(iris_scaled.std())\n",
        "# print(iris_scaled)\n",
        "iris_scaled_df = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
        "\n",
        "print(iris_scaled_df.mean())\n",
        "print(iris_scaled_df.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq_WWaJMSrOt"
      },
      "source": [
        "MinMaxScaler\n",
        "* 데이터값을 0과 1사이의 범위 값으로 변환\n",
        "* 음수인 경우 -1에서 1사이의 값으로 변환\n",
        "* 데이터의 분포가 가우시안 분포가 아닌 경우 Min, Max Scale 적용 가능\n",
        "\n",
        "MinMaxScaler 이용 변환\n",
        "1. MinMaxScaler 객체 생성\n",
        "2. fit()\n",
        "3. transform() : scale 변환된 데이터 셋이 numpy ndarry로 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_H5q88yTGvN"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(iris_df)\n",
        "iris_scaled = scaler.transform(iris_df)\n",
        "# print(iris_scaled)\n",
        "\n",
        "iris_scaled_df = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
        "\n",
        "print(iris_scaled_df.mean())\n",
        "print(iris_scaled_df.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv3NeO9XTbXy"
      },
      "source": [
        "# 각 피처 값 확인\n",
        "iris_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfyCjsjGTfv1"
      },
      "source": [
        "학습 데이터와 테스트 데이터의 스케일링 변환 시 유의점\n",
        "* 학습 데이터와 테스트 데이터의 스케일링 기준 정보 달라지지 않게 주의\n",
        "* 머신러닝 모델은 학습 데이터를 기반으로 학습되기 때문에\n",
        "* 반드시 테스트 데이터는 학습 데이터의 스케일러 기준에 따라야 함\n",
        "* Scaler 객체를 이용해 학습 데이터 세트로 fit()과 transform()을 적용하면\n",
        "* 테스트 데이터에 다시 fit()을 적용해서는 안 되며 \n",
        "* 학습 데이터로 이미 fit()이 적용된 Scaler객체를 이용해 transform()으로 변환해야 함\n",
        "\n",
        "## [참고] 데이터 변환\n",
        "\n",
        "**자료 변환을 통해 자료의 해석을 쉽고 풍부하게 하기 위한 과정**\n",
        "\n",
        "**데이터 변환 목적**\n",
        "- 분포의 대칭화\n",
        "- 산포를 비슷하게\n",
        "- 변수 간의 관계를 단순하게 하기 위해\n",
        "\n",
        "**데이터 변환 종류**\n",
        "- 모양 변환 : pivot, unpivot\n",
        "- 파생변수/요약변수\n",
        "- Normalization (scaling)\n",
        "- 데이터 분포 변환 : 제곱근 변환, 제곱변환, 지수변환, 로그변환, 박스콕스변환"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_ZuLFCLTgby"
      },
      "source": [
        "### 모양변환\n",
        "**Pivot**\n",
        "- 행,열 별 요약된 값으로 정렬해서 분석을 하고자 할 때 사용\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "\n",
        "**Unpivot**\n",
        "- 컬럼 형태로 되어 있는 것을 행 형태로 바꿀 때 사용 (wide form→long form)\n",
        "![image-3.png](attachment:image-3.png)\n",
        "\n",
        "https://support.microsoft.com/ko-kr/office/%ED%94%BC%EB%B2%97-%EC%97%B4-%ED%8C%8C%EC%9B%8C-%EC%BF%BC%EB%A6%AC-abc9c8da-3be9-44c4-886e-0be331ab387a\n",
        "![image-4.png]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXEvWCBeTjyu"
      },
      "source": [
        "### 파생변수/요약변수\n",
        "**파생변수**\n",
        "- 이미 수집된 변수를 활용해 새로운 변수 생성하는 경우\n",
        "- 분석자가 특정 조건을 만족하거나 특정 함수에 의해 값을 만들어 의미를 부여한 변수\n",
        "- 주관적일 수 있으며 논리적 타당성을 갖추어 개발해야 함\n",
        "- 예. 주구매 매장, 구매상품다양성, 가격선호대, 라이프스타일\n",
        "\n",
        "**요약 변수**\n",
        "- 원 데이터를 분석 Needs에 맞게 종합한 변수\n",
        "- 데이터의 수준을 달리하여 종합하는 경우가 많음\n",
        "- 예. 총구매금액, 매장별 방문횟수, 매장이용횟수, 구매상품목록"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqLCCBWNTncv"
      },
      "source": [
        "### 정규화(Normalization)\n",
        "- 단위 차이, 극단값 등으로 비교가 어렵거나 왜곡이 발생할 때, 표준화하여 비교 가능하게 만드는 방법\n",
        "- Scale이 다른 여러 변수에 대해 Scale을 맞춰 모든 데이터 포인트가 동일한 정도의 중요도로 비교되도록 함\n",
        "- Scaling 여부가 모델링의 성능에도 영향을 주기도 함\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "**대표적인 Normalization**\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "https://stats.stackexchange.com/questions/287425/why-do-you-need-to-scale-data-in-knn\n",
        "http://hleecaster.com/ml-normalization-concept/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAqHFLpxTnyd"
      },
      "source": [
        "### 데이터 분포의 변환\n",
        "- 정규분포를 가정하는 분석 기법을 사용할 때 입력데이터가 정규를 따르지 않는 경우, 정규분포 혹은 정규분포에 가깝게 변환하는 기법\n",
        "\n",
        "- Positively Skewed\n",
        "    - $sqrt(x)$\n",
        "    - $log_{10}{(x)}$\n",
        "    - $1 / x$\n",
        "\n",
        "\n",
        "- Negatively Skewed\n",
        "    - $sqrt(max(x+1) – x)$\n",
        "    - $log_{10}(max(x+1) –x)$\n",
        "    - $1 / (max(x+1) - x)$\n",
        "    \n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "https://www.datanovia.com/en/lessons/transform-data-to-normal-distribution-in-r/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFWA385cT-re"
      },
      "source": [
        "# 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeFi2G3nUNNV"
      },
      "source": [
        "## 사이킷런으로 수행하는 타이타닉 생존자 예측\n",
        "\n",
        "* 캐글에서 제공하는 타이타닉 탑승자 데이터 기반으로\n",
        "* 생존자 예측을 사이킷런으로 수행\n",
        "\n",
        "* 타이타닉 생존자 데이터\n",
        "    - 머신러닝에 입문하는 데이터 분석가/과학자를 위한 기초 예제로 제공\n",
        "    - 많은 캐글 이용자가 자신의 방법으로 타이타닉 생존자 예측을 수행하고\n",
        "    - 수행 방법을 캐글에 공유\n",
        "\n",
        "* 캐글 : 데이터 분석 오픈 포탈\n",
        "    - 세계적인 ML 기반 분석 대회를 온라인 상에서 주관\n",
        "    \n",
        "캐글사이트 : https://www.kaggle.com/c/titanic/data\n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWcsQxodUPWH"
      },
      "source": [
        "## 변수 별 정보"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "qMollpgqVYbo",
        "outputId": "83981679-154c-4b1c-e3ef-7866e64afed8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "\n",
        "titanic_df = pd.read_csv('./titanic_train.csv')\n",
        "titanic_df.head(3)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
              "0            1         0       3  ...   7.2500   NaN         S\n",
              "1            2         1       1  ...  71.2833   C85         C\n",
              "2            3         1       3  ...   7.9250   NaN         S\n",
              "\n",
              "[3 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIYqNqKyVvpG"
      },
      "source": [
        "* Passengerid: 탑승자 데이터 일련번호\n",
        "* survived: 생존 여부, 0 = 사망, 1 = 생존\n",
        "* Pclass: 티켓의 선실 등급, 1 = 일등석, 2 = 이등석, 3 = 삼등석\n",
        "* sex: 탑승자 성별\n",
        "* name: 탑승자 이름\n",
        "* Age: 탑승자 나이\n",
        "* sibsp: 같이 탑승한 형제자매 또는 배우자 인원수\n",
        "* parch: 같이 탑승한 부모님 또는 어린이 인원수\n",
        "* ticket: 티켓 번호\n",
        "* fare: 요금\n",
        "* cabin: 선실 번호\n",
        "* embarked: 중간 정착 항구 C = Cherbourg, Q = Queenstown, S = Southampton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2SPSz3_VZDJ"
      },
      "source": [
        "titanic_df.shape"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntE68YiyVfYR"
      },
      "source": [
        "# 데이터 칼럼 타입 확인\n",
        "titanic_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKopNlEtV7Cx"
      },
      "source": [
        "데이터 정보 확인\n",
        "- RangeIndex: 891 entries, 0 to 890 : 전체 행 (891개 행)\n",
        "- Data columns (total 12 columns): 칼럼 수 (12개)\n",
        "- float64 : 2개\n",
        "- int64 : 5개\n",
        "- object(string) : 5개\n",
        " - (판다스는 넘파이 기반으로 만들어졌는데\n",
        " - 넘파이의 String 타입의 길이 제한이 있기 때문에 \n",
        " - 이에 대한 구분을 위해 object 타입으로 명기)\n",
        "- Age : 714개 (Null값(NaN): 177개)\n",
        "- Cabin : 204개 (Null값(NaN): 687개)\n",
        "- Embarked : 889개 (Null값(NaN): 2개)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B57vYSfdV7yc"
      },
      "source": [
        "## 결측치 파악"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TKBml5uWSo4"
      },
      "source": [
        "# Null 값 개수 확인\n",
        "#titanic_df['Age'].isnull()\n",
        "titanic_df['Age'].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utbeqqu9WSdJ"
      },
      "source": [
        "titanic_df['Cabin'].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W20RjiKeWWZ2"
      },
      "source": [
        "titanic_df['Embarked'].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kaHESjEWSCe"
      },
      "source": [
        "##  데이터 전처리 : 결측치 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muCzbVuQWco-"
      },
      "source": [
        "### NULL 컬럼들에 대한 처리\n",
        "\n",
        "- 사이킷 머신러닝 알고리즘은 Null 값을 허용하지 않으므로\n",
        "- Null 값을 어떻게 처리할지 결정\n",
        "- DataFrame()의 fillna() 메서드를 사용해서 \n",
        "- Null 값을 평균 또는 고정 값으로 변경\n",
        "- Age : 평균 나이로 변경\n",
        "- 나머지 칼럼 : 'N'으로 변경"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gegdfvupWdUq"
      },
      "source": [
        "# Null 처리\n",
        "# titanic_df['Age'].mean()\n",
        "titanic_df['Age'].fillna(titanic_df['Age'].mean, inplace=True)\n",
        "titanic_df['Cabin'].fillna(\"N\", inplace = True)\n",
        "titanic_df['Embarked'].fillna('N', inplace =True)\n",
        "\n",
        "# 모든 칼럼의 Null 값을 합산해서 Null 값이 없는지 확인\n",
        "titanic_df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9oXRoJlWvXS"
      },
      "source": [
        "### 문자열 변수(피처) 빈도 분포 확인 : value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pqOFebXW84g"
      },
      "source": [
        "# 문자열 피처 (Sex, Cabin, Embarked) 값 분류 확인\n",
        "print('Sex분포: \\n', titanic_df['Sex'].value_counts())\n",
        "print('\\nCabin분포: \\n', titanic_df['Cabin'].value_counts())\n",
        "print('\\nEmbarked분포: \\n', titanic_df['Embarked'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBfSqKRMW8kc"
      },
      "source": [
        "### 문자열 변수 Cabin값 변경"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsyW9qWyXGza"
      },
      "source": [
        "# Cabin 칼럼 값 중에서 첫 번째 문자만 추출\n",
        "titanic_df['Cabin'].str[:1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWERKzADXS3e"
      },
      "source": [
        "# Cabin 값을 선실등급만으로 표기 (선실 번호 제외)\n",
        "titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]\n",
        "\n",
        "# 선실등급 별 개수 확인\n",
        "titanic_df['Cabin'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxICD5ElXSYj"
      },
      "source": [
        "## 성별에 따른 생존자수 분포(EDA)\n",
        "\n",
        "머신러닝 알고리즘 적용해서 예측 수행 전에 데이터 탐색\n",
        "- 어떤 유형의 승객이 생존 확률이 높았는지 확인\n",
        "- 성별이 생존 확률에 어떤 영향을 미쳤는지 확인\n",
        "- 성별에 따른 생존자 수 비교\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty2GmrfnXgSW"
      },
      "source": [
        "# 성별(Sex) 분포 확인\n",
        "titanic_df.groupby('Sex')['Sex'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j-piAEFXhnB"
      },
      "source": [
        "# 생존(Survived) 분포 확인\n",
        "titanic_df.groupby('Survived')['Survived'].count()\n",
        "# 0 사망, 1 생존"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcEDq2MBXpgo"
      },
      "source": [
        "# 성별(Sex) 생존(Survived) 확인\n",
        "# 사망 : 0\n",
        "# 생존 : 1\n",
        "# Survived 칼럼 : 레이블로 결정 클래스 값\n",
        "\n",
        "titanic_df.groupby(['Sex', 'Survived'])['Survived'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK7EdV_6X5oQ"
      },
      "source": [
        "# 성별 생존자: 막대 그래프 (barplot)\n",
        "sns.barplot(x='Sex', y='Survived', data=titanic_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oodXFP1GYB3o"
      },
      "source": [
        "# 객실 등급별/성별 생존 확률\n",
        "sns.barplot(x='Pclass', y='Survived', hue='Sex', data=titanic_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}